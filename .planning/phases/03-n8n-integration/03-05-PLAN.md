---
phase: 03-n8n-integration
plan: 05
type: execute
---

<objective>
Wire beat-sync data to narration timing in n8n workflow.

Purpose: Enable snare-hit-aligned image transitions during lulls between narration sections, creating punchy, music-synced video transitions.
Output: Updated n8n workflow that uses webhook musicSnareHits[] to time image transitions during narration gaps.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-n8n-integration/03-04-SUMMARY.md

**Current state:**
- ElevenLabs TTS works (Convert text to speech → S3 → Get duration)
- Duration data feeds into json2video for audio timeline placement
- Beat data (musicSnareHits, musicBassHits, musicBpm) is sent in webhook payload
- BUT: Beat data is NOT used for image transition timing

**STATE.md decision (key constraint):**
"Beat-sync in lulls only: Snare-hit transitions happen during gaps between narration sections (not during voice). 4 lulls = 4 beat-synced section transitions. Within sections, soft crossfades between images."

**Gap:**
- `prepare body for jsontovideo to set video` calculates generic gaps between audio
- Image transitions currently happen at arbitrary points
- Need to align image transitions to snare hits DURING the calculated gaps

**Data available at workflow runtime:**
- `$('Webhook').first().json.body[0].musicSnareHits` - array of snare hit timestamps
- `$('Webhook').first().json.body[0].musicBassHits` - array of bass hit timestamps
- `$('Webhook').first().json.body[0].musicBpm` - beats per minute
- Audio durations from `$('Get duration of voices').all()`
- Gap timings calculated in `prepare body for jsontovideo to set video`

**Workflow ID:** hjG60LIO86i5vxX3
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add beat-sync image timing calculation</name>
  <files>n8n workflow node: prepare body for jsontovideo to set video</files>
  <action>
Update the existing "prepare body for jsontovideo to set video" Code node to:

1. Read snare hits from webhook: `$('Webhook').first().json.body[0].musicSnareHits || []`
2. After calculating gaps between audio sections, identify which snare hits fall within each gap
3. For each gap period (between narration end and next narration start):
   - Find snare hits that occur during the gap window
   - Select 1-2 snare hits per gap for image transitions (avoid clustering)
   - Store these as `imageTransitionTimes[]`
4. Add `imageTransitionTimes` to the returned payload for downstream use

Key logic:
```javascript
// After calculating gap timing...
const snareHits = $('Webhook').first().json.body[0].musicSnareHits || [];

// Find snare hits in each lull (gap between narration sections)
const imageTransitionTimes = [];
let gapStart = introSilence; // After intro

clipsBeforeLast.forEach((clip, i) => {
  const audioEnd = gapStart + clip.duration;
  const nextAudioStart = audioEnd + gap;

  // Find snare hits in this gap window
  const hitsInGap = snareHits.filter(t => t > audioEnd && t < nextAudioStart);

  // Take first hit in gap for image transition (avoid multiple transitions per gap)
  if (hitsInGap.length > 0) {
    imageTransitionTimes.push(hitsInGap[0]);
  }

  gapStart = nextAudioStart;
});
```

Include `imageTransitionTimes` in the returned JSON for use by video rendering nodes.
Do NOT change existing audio timeline logic - only ADD the beat-sync calculation.
  </action>
  <verify>
n8n MCP: get_node for "prepare body for jsontovideo to set video" shows imageTransitionTimes calculation.
Node still returns valid JSON payload with scenes array.
  </verify>
  <done>
Code node updated with snare-hit gap detection logic.
imageTransitionTimes array included in output payload.
Existing audio timeline logic preserved.
  </done>
</task>

<task type="auto">
  <name>Task 2: Pass transition times to video metadata node</name>
  <files>n8n workflow node: get video metadata to add music</files>
  <action>
Update "get video metadata to add music" Code node to:

1. Read `imageTransitionTimes` from upstream node:
   `const transitionTimes = $('prepare body for jsontovideo to set video').first().json.imageTransitionTimes || [];`

2. Include in the output object alongside existing durations/starts:
```javascript
return [{
  json: {
    videoUrl,
    musicUrl,
    durations,
    starts,
    videoDuration,
    imageTransitionTimes: transitionTimes  // ADD this field
  }
}];
```

This passes the beat-synced transition times to the final render node for future use in image crossfade timing.

Do NOT modify any existing fields - only ADD the new field to passthrough.
  </action>
  <verify>
n8n MCP: get_node shows imageTransitionTimes passthrough in output.
Existing musicUrl, durations, starts, videoDuration fields unchanged.
  </verify>
  <done>
imageTransitionTimes passed through to render preparation.
All existing fields preserved.
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>Beat-sync timing calculation wired through workflow</what-built>
  <how-to-verify>
1. Open n8n workflow: https://edgeaimedia.app.n8n.cloud/workflow/hjG60LIO86i5vxX3
2. Click "prepare body for jsontovideo to set video" node
3. Verify code includes:
   - Reading `musicSnareHits` from Webhook
   - Calculating `imageTransitionTimes` array
   - Including it in return payload
4. Click "get video metadata to add music" node
5. Verify `imageTransitionTimes` is passed through in output
6. Optional: Test with a sample execution to verify no errors
  </how-to-verify>
  <resume-signal>Type "approved" to continue, or describe any issues</resume-signal>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] `prepare body for jsontovideo to set video` reads musicSnareHits from webhook
- [ ] imageTransitionTimes calculated from snare hits in narration gaps
- [ ] `get video metadata to add music` passes imageTransitionTimes through
- [ ] Existing audio timeline logic unchanged
- [ ] No n8n workflow errors introduced
</verification>

<success_criteria>

- Beat-sync timing data flows from webhook → json2video nodes
- Snare hits in narration gaps identified as transition points
- Data available for future image crossfade enhancement
- No regression in existing TTS/audio timeline functionality
  </success_criteria>

<output>
After completion, create `.planning/phases/03-n8n-integration/03-05-SUMMARY.md`:

# Phase 03 Plan 05: ElevenLabs Narration with Timing Summary

**[Substantive one-liner]**

## Accomplishments

- [Key outcome]

## Files Created/Modified

- n8n workflow nodes modified

## Decisions Made

[If any]

## Issues Encountered

[If any]

## Next Phase Readiness

Ready for 03-06-PLAN.md (json2video dual-render)
</output>
